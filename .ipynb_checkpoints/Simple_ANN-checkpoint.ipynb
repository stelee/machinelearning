{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Implementation Of Artificial Neural Network\n",
    "\n",
    "Some one said the best way to learn is to hack. Since I have started to learn Machine Learning for several months, I decided to do something more interesting, instead of to copy the code from book. Inspired from the following links:\n",
    "\n",
    "https://iamtrask.github.io/2015/07/27/python-network-part2/\n",
    "\n",
    "and(specially)\n",
    "\n",
    "https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1\n",
    "\n",
    "I think maybe it is not a bad idea to implement a simple ANN by myself, with less mathematics and more human understoodable way.\n",
    "\n",
    "## Make some import and prepare the training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([\n",
    "    [0,1,0],\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [1,1,0],\n",
    "    [0,1,1],\n",
    "    [1,0,1]\n",
    "])\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is simple. Input will be a series of 0 and 1. For each input, if the number of *1* is bigger than *0*,  it will return *1*, else it will return *0*. It is very simple idea and even one layer of ANN can complete the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the hypothetical and cost method\n",
    "\n",
    "The first step is to define the hypothetical method with weights as input. the weights is the measurement for each x element for input X and is also our goal to optimize. The hypothetical method is using sigmoid to do the prediction.\n",
    "\n",
    "$$h = 1/(1+ e^{-x})$$\n",
    "A mathimatics question, why sigmoid?\n",
    "\n",
    "Because it logically simulate the real word when we make the decision. And give enough spread when we are far from making the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h(weights):\n",
    "    return 1/(1+np.exp(-X.dot(weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define the cost method. The goal for optimize the weights is to make the return value of cost value as small as possible. \n",
    "So far the code is very similar as regular classification. Actually, it is a regular classification.\n",
    "\n",
    "$$cost=\\sum(y' - y)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(weights):\n",
    "    return np.sum(np.square(h(weights) - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why square? Mmm... it is just make the difference more obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descend with no math\n",
    "\n",
    "There are a lot of way to do the gradient decend to get the optima. Some of them are related with mathmatics. For example, for sigmoid prediction, you can use derivative formula:\n",
    "\n",
    "$$\\frac{d\\sigma (x)}{d(x)} = \\sigma (x)\\cdot (1-\\sigma(x)).$$\n",
    "\n",
    "But I am going to try something simpler for the reason:\n",
    "\n",
    "* there are still a lot of scenarios when a simple formula can't match to get the derivative\n",
    "* I want to make this artical more interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_for_each(weights, index):\n",
    "    small_value_change = 0.001\n",
    "    new_weights = np.copy(weights) # this is import, not to change the original weights\n",
    "    new_weights[index, 0] = new_weights[index, 0] + small_value_change;\n",
    "    return (cost(new_weights) -  cost(weights))/small_value_change\n",
    "\n",
    "def diff(weights):\n",
    "    return np.array([ diff_for_each(weights, index) for index, _ in enumerate(weights)]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *diff* method will be used to caculate the difference matrix when one single weight is changed in the weights matrix.\n",
    "\n",
    "Now we can start to train our single layer ANN network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10000\n",
    "np.random.seed(1)\n",
    "m, n = X.shape\n",
    "weights = 2 * np.random.random((4, 1)) - 1\n",
    "X = np.c_[np.ones((m,1)),X] #add bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "for i in range(steps):\n",
    "    weights = weights - diff(weights) * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also need to create a predict method to give the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    return 1/(1+np.exp(-X.dot(weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's cross check our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01019554],\n",
       "       [ 0.01019554],\n",
       "       [ 0.01019554],\n",
       "       [ 0.99116804],\n",
       "       [ 0.99116804],\n",
       "       [ 0.99116804]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is nice, as for the first three input, the average rate to have output as 1 is 1%, which means the out put is 0 and the last three outputs have 99% chance to be 1!\n",
    "\n",
    "Let's try some test data\n",
    "[1,1,1] -> 1\n",
    "[0,0,0] -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99999918]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(np.array([[1,1,1,1]])) # don't forget the first input will be a bias input and always be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.45432964e-07]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(np.array([[1,0,0,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is almost the end of the exploring of the single layer ANN. It is working, but not a workable file we can re-use. The following I will wrap this up in a class, do the training and plot the cost history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SingleLayerNN():\n",
    "    def __init__(self, training_steps = 1000, small_diff_value = 0.001, seed = 1, record_cost_history = False, learning_rate = 1):\n",
    "        self.training_steps = training_steps\n",
    "        self.small_diff_value =  small_diff_value\n",
    "        self.seed = seed\n",
    "        self.learning_rate = learning_rate\n",
    "        self.record_cost_history = record_cost_history\n",
    "        self.cost_history = []\n",
    "            \n",
    "    def __h(self,X, weights):\n",
    "        return 1/(1+np.exp(-X.dot(weights)))\n",
    "    \n",
    "    def __cost(self,weights):\n",
    "        return np.sum(np.square(self.__h(self.X_, weights) - y))\n",
    "    \n",
    "    def __diff_for_each(self,index):\n",
    "        new_weights = np.copy(self.weights_) # this is import, not to change the original weights\n",
    "        new_weights[index, 0] = new_weights[index, 0] + self.small_diff_value\n",
    "        return (self.__cost(new_weights) -  self.__cost(self.weights_))/self.small_diff_value\n",
    "\n",
    "    def __diff(self):\n",
    "        return np.array([ self.__diff_for_each(index) for index, _ in enumerate(self.weights_)]).reshape(-1,1)\n",
    "    \n",
    "    def fit(self,X, y):\n",
    "        m, n = X.shape\n",
    "        np.random.seed(self.seed)\n",
    "        self.weights_ = 2 * np.random.random((n+1, 1)) - 1\n",
    "        self.X_ = np.c_[np.ones((m,1)),X]\n",
    "        self.y_ = y\n",
    "        for i in range(self.training_steps):\n",
    "            self.weights_ = self.weights_ - self.__diff() * self.learning_rate\n",
    "            if self.record_cost_history:\n",
    "                self.cost_history.append(self.__cost(self.weights_))\n",
    "            \n",
    "    def predict(self,X):\n",
    "        m, _ = X.shape\n",
    "        X_to = np.c_[np.ones((m,1)),X]\n",
    "        return self.__h(X_to, self.weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = SingleLayerNN(record_cost_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0,1,0],\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [1,1,0],\n",
    "    [0,1,1],\n",
    "    [1,0,1]\n",
    "])\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03373278],\n",
       "       [ 0.03373278],\n",
       "       [ 0.03373278],\n",
       "       [ 0.97083048],\n",
       "       [ 0.97083048],\n",
       "       [ 0.97083048]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM2tCDnczsyb0/wEmWGRmwzm1wQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1067132b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(clf.cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99968485e-01],\n",
       "       [  3.66167486e-05]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(np.array([\n",
    "    [1,1,1],\n",
    "    [0,0,0]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
